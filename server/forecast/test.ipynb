{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from forecast.games_dataset_features import *\n",
    "\n",
    "Dataset = namedtuple('Dataset', ['train_x', 'train_y', 'test_x', 'test_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6615, 107)\n(6615,)\n(6615,)\n"
     ]
    }
   ],
   "source": [
    "dataset_file = 'games_10_3'\n",
    "x_origin = np.load('./server/forecast/dataset/{}_x.npy'.format(dataset_file))\n",
    "y = np.load('./server/forecast/dataset/{}_y.npy'.format(dataset_file))\n",
    "\n",
    "print(x_origin.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# y2 is a target for win/lose only\n",
    "y2 = y.copy()\n",
    "y2[y2 == 1] = 0\n",
    "y2[y2 == 2] = 1\n",
    "y2[y2 == 3] = 1\n",
    "print(y2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106]\n"
     ]
    }
   ],
   "source": [
    "x_data = np.zeros((x_origin.shape[0], (x_origin.shape[1] + 1)//2), dtype=x_origin.dtype)\n",
    "print(x_data.shape)\n",
    "\n",
    "x_data[:, FEATURE_IS_REGULAR] = x_origin[:, FEATURE_IS_REGULAR]\n",
    "x_data[:, FEATURE_HOME_DAYS_SINCE_LAST_GAME:] = \\\n",
    "    x_origin[:, FEATURE_HOME_DAYS_SINCE_LAST_GAME:FEATURE_AWAY_DAYS_SINCE_LAST_GAME] -\\\n",
    "    x_origin[:, FEATURE_AWAY_DAYS_SINCE_LAST_GAME:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 66, 67, 36, 100, 101, 72, 106, 13, 14, 47, 48, 19, 83, 53, 84, 89, 58, 30, 31}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "sel = VarianceThreshold(threshold=0.1)\n",
    "sel.fit(x_origin)\n",
    "\n",
    "all_idx = set(range(FEATURE_COUNT))\n",
    "idx = set(sel.get_support(indices=True))\n",
    "print(all_idx.difference(idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def split_dataset(features, target, test_data_percentage=0.25, norm=False):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=test_data_percentage)\n",
    "    if norm:\n",
    "        scaler = StandardScaler()  \n",
    "        scaler.fit(x_train)  \n",
    "        x_train = scaler.transform(x_train)  \n",
    "        x_test = scaler.transform(x_test)  \n",
    "    return Dataset(x_train, y_train, x_test, y_test)\n",
    "\n",
    "\n",
    "def evaluate_model(model, data):\n",
    "    predictions = model.predict(data.test_x)\n",
    "    print(\"Train Accuracy: \", accuracy_score(data.train_y, model.predict(data.train_x)))\n",
    "    print(\"Test Accuracy: \", accuracy_score(data.test_y, predictions))\n",
    "    print(\"Confusion matrix\\n\", confusion_matrix(data.test_y, predictions))\n",
    "\n",
    "\n",
    "def hyperparameters_tune(model, features, target, params, iter_num=20, cv=3):\n",
    "    rscv = RandomizedSearchCV(estimator=model, param_distributions=params,\n",
    "                              n_iter=iter_num, cv=cv, n_jobs=-1)\n",
    "    rscv.fit(features, target)\n",
    "    return rscv\n",
    "\n",
    "\n",
    "def feature_importance(clf):\n",
    "    importances = list(clf.feature_importances_)\n",
    "    feature_importances =\\\n",
    "        [(f, round(imp, 2)) for f, imp in zip(range(FEATURE_COUNT), importances)]\n",
    "    feature_importances = sorted(feature_importances, key=lambda x: x[1], reverse=True)\n",
    "    [print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASKS = {\n",
    "    'all': None,\n",
    "    'L_H2H': FEATURES_L_H2H,\n",
    "    'S_H2H': FEATURES_S_H2H,\n",
    "    'H2H': FEATURES_H2H\n",
    "}\n",
    "\n",
    "TARGETS = {\n",
    "    '4Classes': y,\n",
    "    '2Classes': y2\n",
    "}\n",
    "\n",
    "\n",
    "def test_feature_sets(train_func, data=None, test_data_percentage=0.25, norm=False):\n",
    "    if data is not None:\n",
    "        for target in TARGETS.keys():\n",
    "            print('Target {}'.format(target))\n",
    "            dataset = split_dataset(data, TARGETS[target], test_data_percentage=test_data_percentage, norm=norm)\n",
    "            train_func(dataset)\n",
    "    else:        \n",
    "        for feature_mask in MASKS.keys():\n",
    "            if MASKS[feature_mask] is None:\n",
    "                x = x_origin\n",
    "            else:\n",
    "                x = x_origin[:, MASKS[feature_mask]]\n",
    "            for target in TARGETS.keys():\n",
    "                print('Features {}, target {}'.format(feature_mask, target))\n",
    "                dataset = split_dataset(x, TARGETS[target], test_data_percentage=test_data_percentage, norm=norm)\n",
    "                train_func(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def train_random_forest(data):\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [300, 400]\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['auto', 20, 30]\n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [None, 5, 10, 20]\n",
    "    # Minimum number of samples required to split a node\n",
    "    min_samples_split = [10]\n",
    "    # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [4]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    \n",
    "    rf_param_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'max_features': max_features,\n",
    "        'max_depth': max_depth,\n",
    "        'min_samples_split': min_samples_split,\n",
    "        'min_samples_leaf': min_samples_leaf,\n",
    "        'bootstrap': bootstrap\n",
    "    }\n",
    "    \n",
    "    rf_model = RandomForestClassifier()\n",
    "    rand_cv = hyperparameters_tune(rf_model, data.train_x, data.train_y, rf_param_grid, iter_num=20, cv=5)\n",
    "    \n",
    "    print(rand_cv.best_params_)\n",
    "    evaluate_model(rand_cv.best_estimator_, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "def train_ada_boost(data):\n",
    "    n_estimators = [50, 100, 400, 600]\n",
    "    learning_rate = [0.1, 0.5, 1.0, 1.3, 1.5]    \n",
    "\n",
    "    ab_param_grid = {\n",
    "        'n_estimators': n_estimators,\n",
    "        'learning_rate': learning_rate\n",
    "    }\n",
    "    \n",
    "    ab_model = AdaBoostClassifier()\n",
    "    rand_cv = hyperparameters_tune(ab_model, data.train_x, data.train_y, ab_param_grid, iter_num=20, cv=5)\n",
    "    \n",
    "    print(rand_cv.best_params_)\n",
    "    evaluate_model(rand_cv.best_estimator_, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "def train_naive_bayes(data):\n",
    "    nb_param_grid = {\n",
    "        'priors': [None]\n",
    "    }\n",
    "    \n",
    "    nb_model = GaussianNB()\n",
    "    rand_cv = hyperparameters_tune(nb_model, data.train_x, data.train_y, nb_param_grid, iter_num=1, cv=5)\n",
    "    \n",
    "    print(rand_cv.best_params_)\n",
    "    print(rand_cv.best_estimator_.class_prior_)\n",
    "    evaluate_model(rand_cv.best_estimator_, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "def train_mlp(data):\n",
    "    mlp_param_grid = {\n",
    "        'hidden_layer_sizes': [(5,), (25,), (30, 10), (25, 10), (20, 5)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.00001, 0.0001, 1.0, 1.5],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'max_iter': [1200, 1500]\n",
    "    }\n",
    "\n",
    "    mlp = MLPClassifier()\n",
    "    rand_cv = hyperparameters_tune(mlp, data.train_x, data.train_y, mlp_param_grid, iter_num=80, cv=5)\n",
    "    \n",
    "    print(rand_cv.best_params_)\n",
    "    evaluate_model(rand_cv.best_estimator_, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features all, target 4Classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 400, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.4696633743196936\nTest Accuracy:  0.4347037484885127\nConfusion matrix\n [[571   0   3 131]\n [157   0   0  45]\n [135   0   1  53]\n [411   0   0 147]]\nFeatures all, target 2Classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'learning_rate': 0.1}\nTrain Accuracy:  0.5908083047772626\nTest Accuracy:  0.5749697702539298\nConfusion matrix\n [[738 172]\n [531 213]]\nFeatures L_H2H, target 4Classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 600, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.468655513001411\nTest Accuracy:  0.4480048367593712\nConfusion matrix\n [[567   1   0 124]\n [146   0   0  50]\n [122   0   0  60]\n [410   0   0 174]]\nFeatures L_H2H, target 2Classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'learning_rate': 0.1}\nTrain Accuracy:  0.589397298931667\nTest Accuracy:  0.5640870616686819\nConfusion matrix\n [[761 151]\n [570 172]]\nFeatures S_H2H, target 4Classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 400, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.4611973392461197\nTest Accuracy:  0.43893591293833134\nConfusion matrix\n [[581   1   0 129]\n [158   0   0  43]\n [142   0   0  38]\n [417   0   0 145]]\nFeatures S_H2H, target 2Classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'learning_rate': 0.5}\nTrain Accuracy:  0.614190687361419\nTest Accuracy:  0.5423216444981862\nConfusion matrix\n [[635 251]\n [506 262]]\nFeatures H2H, target 4Classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 400, 'learning_rate': 0.1}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.4464825639991937\nTest Accuracy:  0.44316807738814995\nConfusion matrix\n [[605   2   0  89]\n [158   0   0  38]\n [147   0   0  23]\n [464   0   0 128]]\nFeatures H2H, target 2Classes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 50, 'learning_rate': 0.5}\nTrain Accuracy:  0.5928240274138279\nTest Accuracy:  0.5532043530834341\nConfusion matrix\n [[700 207]\n [532 215]]\n"
     ]
    }
   ],
   "source": [
    "test_feature_sets(train_ada_boost, norm=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.5982664785325539\nTest Accuracy:  0.5761789600967352\nConfusion matrix\n [[693 207]\n [494 260]]\n"
     ]
    }
   ],
   "source": [
    "dataset = split_dataset(x_data, y2, norm=True)\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, ),\n",
    "                    activation=\"relu\",\n",
    "                    solver='adam',\n",
    "                    alpha=2.0,\n",
    "                    learning_rate=\"adaptive\",\n",
    "                    learning_rate_init=0.0001,\n",
    "                    max_iter=800)\n",
    "\n",
    "mlp.fit(dataset.train_x, dataset.train_y)\n",
    "evaluate_model(mlp, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show loss curve\n",
    "x_axis = range(0, len(mlp.loss_curve_))\n",
    "plt.plot(x_axis, mlp.loss_curve_, 'ko-')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
